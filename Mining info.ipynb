{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 85.0.4183\n",
      "[WDM] - Get LATEST driver version for 85.0.4183\n",
      "[WDM] - Driver [C:\\Users\\conex_3\\.wdm\\drivers\\chromedriver\\win32\\85.0.4183.87\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://vk.com/otchisleno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element_by_class_name('ui_tab_search')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = driver.find_element_by_id('wall_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.clear()\n",
    "row.send_keys('девушкой\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def scroll_to_the_end(driver=driver):\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        pause = np.abs(np.random.normal()) + 0.2\n",
    "        time.sleep(pause)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "scroll_to_the_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = bs.findAll('div', attrs={'class': 'post_content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_posts_girls(posts):\n",
    "    y = lambda x: str(x).lower()\n",
    "    return [x for x in posts if u'девушк' in y(x) and u'твоя' in y(x) and u'ты' not in y(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts), len(filter_posts_girls(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls = filter_posts_girls(posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"wall_post_text zoom_text\">если бы твоя девушка была вузом то это был бы единственный вуз в который все ходили</div>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "girls[0].findAll('div', attrs={'class': 'wall_post_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "row.clear()\n",
    "row.send_keys('девушка \\n')\n",
    "\n",
    "time.sleep(2.3)\n",
    "scroll_to_the_end()\n",
    "\n",
    "bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "posts = bs.findAll('div', attrs={'class': 'post_content'})\n",
    "def filter_posts_girls_month(posts):\n",
    "    return [x for x in posts if u'девушк' in str(x).lower()]\n",
    "len(filter_posts_girls_month(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "morph_vocab = MorphVocab()\n",
    "extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "def get_texts(posts):\n",
    "    t = []\n",
    "    for x in posts:\n",
    "        t.extend(list(map(lambda z: z.text, x.findAll('div', attrs={'class': 'wall_post_text'}))))\n",
    "    t = [x for x in t]\n",
    "    return t\n",
    "\n",
    "\n",
    "def save_texts(texts, name='data.txt'):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('\\n'.join(texts))\n",
    "\n",
    "\n",
    "def load_texts(name='data.txt'):\n",
    "    with open(name) as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_texts(girls)\n",
    "save_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://vk.com/search?c%5Bgroup%5D=161102237&c%5Bsection%5D=people')\n",
    "sex = driver.find_element_by_xpath(\"//div[contains(@onclick,'radiobtn')]\")\n",
    "sex.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scroll_to_the_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def get_refs2persons(page):\n",
    "    return ['vk.com' + x.findAll('a')[0].get_attribute_list('href')[0] for x in page.findAll('div', attrs={'class': 'labeled name'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = get_refs2persons(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_texts(candidates, 'candidates.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.liveinternet.ru/users/4468278/post246073969/')\n",
    "\n",
    "bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "compliments = [x for x in bs.findAll('p')]\n",
    "compliments = [x for x in compliments if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compliments = [x.text.strip() for x in compliments]\n",
    "compliments = [x for x in compliments if len(x) > 0]\n",
    "compliments = [' '.join(word_tokenize(x)) for x in compliments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    tokenizer = RegexpTokenizer(r'[а-я]+|[А-Я]+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [morph.parse(w)[0].normal_form for w in tokens if morph.tag(w)[0].POS == 'ADJF']\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for c in compliments:\n",
    "    tmp.extend(preprocess(c))\n",
    "tmp = list(set(tmp) ^ {\n",
    "    'наш', \n",
    "    'свой', \n",
    "    'общий', \n",
    "    'непостоянный', \n",
    "    'беспечный', \n",
    "    'эгоистичный', \n",
    "    'сумасшедший', \n",
    "    'весь', \n",
    "    'каждый',\n",
    "    'индивидуальный',\n",
    "    'огнеопасный',\n",
    "    'упоенный',\n",
    "    'другой',\n",
    "    'шипучий',\n",
    "    'тот'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-daaf3eea6321>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
